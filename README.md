# Vision-Language Chatbot with BLIP, RAG + Tool-Using Agent

## Problem Statement & Overview

Modern image captioning and visual QA systems often lack real-world knowledge or context-awareness. While they may describe what they "see" in an image, they can’t explain it or answer open-ended questions about it in depth - especially when such knowledge isn’t directly visible.

This project addresses that gap by building a **streamlit-based multimodal chatbot** that understands images using **BLIP** (Bootstrapped Language-Image Pretraining), then intelligently selects tools to:
- Generate captions
- Answer visual questions
- Compare captions to verify user input
- Look up factual knowledge from Wikipedia
- Perform RAG (retrieval-augmented generation) to provide detailed explanations

The chatbot acts like an **AI Agent**, using a lightweight LLM (hosted on [Groq](https://groq.com/)) to decide which specialized tool should be invoked to respond best to user input.

### Goals:
- Build an intelligent image chatbot that feels interactive and smart
- Seamlessly combine **image understanding**, **external factual retrieval**, and **LLM-based reasoning**
- Explore real-world tool-use reasoning by an LLM Agent


## Methodology

This project integrates vision-language reasoning using a combination of the following:

---

### BLIP Transformers for Image Understanding Tasks

- **Image Captioning** with `Salesforce/blip-image-captioning-base`
- **Visual Question Answering (VQA)** with `Salesforce/blip-vqa-base`
- **Image-Text Matching** with `Salesforce/blip-itm-base-coco`

---

### RAG (Retrieval-Augmented Generation) Pipeline

- Uses the **image caption** to fetch Wikipedia context using the `wikipedia` Python library
- Combines **visual + factual data** to answer deep questions with an LLM

---

### AI Agent for Tool Selection

- Uses a Groq-hosted LLaMA3-8B-Instruct model to analyze user input
- Dynamically selects the appropriate tool from:
  - `caption`, `vqa`, `compare_caption`, `get_info`, `rag_answer`
- Enables an adaptive, modular response pipeline

---

### Streamlit Chat Interface

- Simple, user-friendly chat UI for uploading images and interacting with the AI assistant
- Maintains **stateful conversation**
- Displays all intermediate outputs (captions, Wikipedia info, answers, etc.)

---

### Techniques from the Course Applied

- **Multimodal Learning**: Combining text and vision input for richer AI reasoning
- **Transformer Architectures**: BLIP and LLaMA3 are both state-of-the-art transformer-based models
- **Agent-based Reasoning**: The agent acts as an orchestrator using LLM-driven decision-making
- **RAG for Context Injection**: Extending the model’s knowledge using external retrieval (Wikipedia)

## Implementation & Code

This project is implemented as a modular, explainable system. The core logic is written in Python using the following files:

### Code Structure

- **`app.py`** — Streamlit interface to upload images and chat with the AI agent.
- **`agent.py`** — Hosts the agent logic and uses a Groq-hosted LLaMA3 model to decide which tool to invoke.
- **`blip_utils.py`** — Encapsulates vision-language reasoning using BLIP (image captioning, VQA, image-text matching).
- **`rag_utils.py`** — Handles Wikipedia retrieval and open-ended generation via RAG (Retrieval-Augmented Generation).

### Live Demo

You can run the full system locally with:

```bash
streamlit run app.py

### **Tool Selection Prompt**

def decide_tool(user_message):
    prompt = f"""
You are an intelligent tool selector for a visual assistant. Your job is to decide which specialized function (tool) should be used to respond to the user's message, based on the content of their query and the image provided.

You must choose from the following tools:

- caption: Use this when the user wants a simple description or summary of what is visible in the image. Triggers include words like "describe", "what's in this image", or "what do you see".

- vqa: Use this when the user asks a direct question about something specific in the image — such as objects, colors, actions, or quantities. Triggers include "how many", "is there a", "what color", "who is", "where is", "do you see".

- get_info: Use this when the user asks for factual information about something seen in the image (like an object, place, or person) — especially if the assistant should look up external knowledge. Triggers include "tell me about", "what is", "give info on", "Wikipedia".

- compare_caption: Use this to compare the user given caption [this will be a caption for the image] with the one that will be generated by BLIP in our application. Triggers include description or caption of an image, or something like this in the user message "This image is about"

- rag_answer: Use this when the user is asking a deeper or broader question that needs understanding the image **and** do not rely much on the external information, it is usually trash and unrelated. These are usually open-ended questions like "why is this important", "explain this scene", or "what's happening and why" or, Maybe some other questions that do not fall in the above mentioned category.

Rules:
- ALWAYS return exactly one tool name.
- Do NOT explain your reasoning.
- If the input is vague, default to 'caption'.

User message: \"{user_message.strip()}\"
Tool:
"""
```

## Assessment & Evaluation

### Tool Selection Evaluation

The AI agent’s ability to choose the correct tool was tested through various types of user inputs:

| Input Example                          | Expected Tool     | Tool Selected |
|---------------------------------------|-------------------|---------------|
| "What do you see in this image?"      | caption           | caption    |
| "How many people are there?"         | vqa               | vqa        |
| "Tell me about the monument"         | get_info          | get_info   |
| "This image is about climate change" | compare_caption   | x caption |
| "Why is this scene important?"       | rag_answer        | rag_answer |

The agent performs well in most cases when user queries are clearly phrased. The prompt engineering and use of LLaMA3 hosted via Groq allow for nuanced decisions.



### Output Validation

- **Captioning:** Captions align well with the image content.
- **VQA:** BLIP is able to answer direct questions with reasonable accuracy.
- **Wikipedia Info:** When captions yield recognizable terms, the retrieved summaries are relevant and accurate.
- **RAG Response:** Multi-source answers show effective blending of visual and factual content.
- **Compare Caption:** Matches are assessed using a similarity score. Helpful in checking human descriptions vs model-generated ones.

---

### Known Limitations

- **Wikipedia Retrieval:** Not all image captions yield relevant Wikipedia pages.
- **Agent Overlap:** Some user queries could validly trigger more than one tool (e.g., factual + deep reasoning).
- **Model Hallucinations:** LLMs may occasionally include irrelevant or incorrect details if context is weak or ambiguous.
- **Single-image Input:** Current version supports only one image at a time for interaction.
- **Caption Comparison Ambiguity:** In the `compare_caption` tool, when users phrase their input like “This image is about...” the agent sometimes misinterprets it as a caption to be generated rather than comparing it against the user-provided text. This ambiguity can lead to unexpected behavior if the agent treats the message as a generic caption query instead of a matching task.

## Model & Data Cards

### Model: BLIP (Bootstrapped Language Image Pretraining)

- **Architecture:** Vision-Language Transformer using ViT (Vision Transformer) + GPT2-style decoder.
- **Variants used:**
  - `Salesforce/blip-image-captioning-base`
  - `Salesforce/blip-vqa-base`
  - `Salesforce/blip-itm-base-coco`
- **Trained on:** Public datasets like COCO, Conceptual Captions, and Visual Genome.
- **Use Cases:** Image captioning, Visual Question Answering, Image-Text Matching.
- **License:** BSD-3-Clause (per Salesforce Research)


### Model: LLaMA 3 (via Groq API)

- **Architecture:** Decoder-only transformer, successor to LLaMA 2 by Meta.
- **Version used:** `llama3-8b-8192` (8 billion parameters).
- **Hosted on:** [Groq API](https://console.groq.com/)
- **Use Case:** Tool selection + open-ended reasoning (via RAG).
- **Advantages:**
  - **Speed:** Groq runs inference at high throughput (~300+ tokens/sec).
  - **Open-weight model:** Better transparency & community support.
- **License:** Open model hosted under Groq terms.


### Data Sources

- **Images:** Provided manually by the user via upload.
- **Wikipedia:** Queried live using [`wikipedia`](https://pypi.org/project/wikipedia/) Python library.
- **Generated Data:** Captions and reasoning responses are generated on-the-fly.


### Ethical & Bias Considerations

- **Bias in Pretrained Models:** Both BLIP and LLaMA3 may reflect biases in their training datasets (e.g., cultural or visual stereotypes).
- **Fact Reliability:** Wikipedia content may not always be authoritative or fully up-to-date.
- **Image Misinterpretation:** Captions/VQA results may occasionally hallucinate or misidentify objects in complex images.
- **Privacy & Safety:** Uploaded images are processed locally and not stored.

## Critical Analysis

### Impact & Significance

This project demonstrates how multimodal AI can make visual content more interactive and informative through natural language. It blends cutting-edge models with thoughtful orchestration — enabling users to ask meaningful questions about images, receive grounded answers, and validate their own captions. It lowers the barrier to understanding complex visual scenes.

By connecting image content with external factual knowledge (Wikipedia), the assistant becomes more than a caption generator — it becomes a teacher, analyst, and search tool.

---

### Key Learnings

- **Prompt engineering is everything** — the quality and clarity of the LLM’s output heavily depends on how tasks are framed.
- **Tool orchestration via agents** enables dynamic workflows instead of one-size-fits-all models.
- **Multimodal alignment is tough** — models like BLIP may hallucinate, and comparing user descriptions with generated captions isn’t always straightforward.

---

### ⚠Limitations

- **Caption matching ambiguity**: The compare_caption tool may misinterpret a user-supplied caption as a general message instead of a candidate for comparison — especially if it starts with phrases like “This image is about…”. This can cause the agent to default to “caption” instead of evaluating similarity.
- **RAG reliability**: Wikipedia retrieval works well when the caption mentions known terms, but struggles with abstract or artistic scenes.
- **Groq-specific**: LLaMA3 reasoning is hosted on Groq — inference is fast and free, but requires internet access and an API key.

---

### Next Steps

- Improve caption comparison by parsing user input structure more intelligently (e.g., use regex or prompt enhancements).
- Add self-evaluation feedback loop to let users rate tool accuracy and model responses.
- Fine-tune smaller LLMs locally for offline support or privacy-sensitive use cases.
- Expand retrieval to include additional sources beyond Wikipedia (e.g., Bing, custom corpora).


## Documentation & Resource Links

### Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone https://github.com/your-username/vision-language-agent.git
   cd vision-language-agent

2. **Create Virtual Environment and Install Requirements**
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install -r requirements.txt

3. **Set Up Environment Variables and then run the app**
cp .env.example .env
GROQ_API_KEY=your-api-key-here

streamlit run app.py

### Key Resources & Citations

#### Vision-Language Models
- [BLIP: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2201.12086) — Salesforce Research
- [BLIP GitHub Repository](https://github.com/salesforce/BLIP) — Official BLIP models for captioning, VQA, and image-text matching.

#### Language Models
- [LLaMA 3 by Meta AI](https://ai.meta.com/blog/meta-llama-3/) — State-of-the-art open-weight large language model (LLaMA3-8B used here).
- [Groq Inference Platform](https://console.groq.com/docs) — Ultra-low-latency inference for LLaMA, Mixtral, and others.

#### Wikipedia + Retrieval
- [Wikipedia Python Library](https://pypi.org/project/wikipedia/) — Programmatic access to Wikipedia summaries used in the RAG module.

#### Agents & Reasoning
- [LangChain: Tool Use & Agents](https://python.langchain.com/) — Framework for building multi-tool LLM-powered agents.

---

### Additional References

- [Transformers Library (HuggingFace)](https://huggingface.co/docs/transformers/index)
- [Streamlit Docs](https://docs.streamlit.io/) — Used for building the chat interface.
- [Python-dotenv](https://pypi.org/project/python-dotenv/) — For environment variable handling.

---

### Useful Links

- [Groq API Key Setup](https://console.groq.com/)
- [Salesforce BLIP Model Hub](https://huggingface.co/Salesforce)
- [LLaMA 3 License](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
- [Wikipedia Disambiguation Handling](https://wikipedia.readthedocs.io/en/latest/code.html#wikipedia.DisambiguationError)

---



