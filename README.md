# Vision-Language Chatbot with BLIP, RAG + Tool-Using Agent

## Problem Statement & Overview

Modern image captioning and visual QA systems often lack real-world knowledge or context-awareness. While they may describe what they "see" in an image, they can’t explain it or answer open-ended questions about it in depth - especially when such knowledge isn’t directly visible.

This project addresses that gap by building a **streamlit-based multimodal chatbot** that understands images using **BLIP** (Bootstrapped Language-Image Pretraining), then intelligently selects tools to:
- Generate captions
- Answer visual questions
- Compare captions to verify user input
- Look up factual knowledge from Wikipedia
- Perform RAG (retrieval-augmented generation) to provide detailed explanations

The chatbot acts like an **AI Agent**, using a lightweight LLM (hosted on [Groq](https://groq.com/)) to decide which specialized tool should be invoked to respond best to user input.

### Goals:
- Build an intelligent image chatbot that feels interactive and smart
- Seamlessly combine **image understanding**, **external factual retrieval**, and **LLM-based reasoning**
- Explore real-world tool-use reasoning by an LLM Agent


## Methodology

This project integrates vision-language reasoning using a combination of the following:

---

### BLIP Transformers for Image Understanding Tasks

- **Image Captioning** with `Salesforce/blip-image-captioning-base`
- **Visual Question Answering (VQA)** with `Salesforce/blip-vqa-base`
- **Image-Text Matching** with `Salesforce/blip-itm-base-coco`

---

### RAG (Retrieval-Augmented Generation) Pipeline

- Uses the **image caption** to fetch Wikipedia context using the `wikipedia` Python library
- Combines **visual + factual data** to answer deep questions with an LLM

---

### AI Agent for Tool Selection

- Uses a Groq-hosted LLaMA3-8B-Instruct model to analyze user input
- Dynamically selects the appropriate tool from:
  - `caption`, `vqa`, `compare_caption`, `get_info`, `rag_answer`
- Enables an adaptive, modular response pipeline

---

### Streamlit Chat Interface

- Simple, user-friendly chat UI for uploading images and interacting with the AI assistant
- Maintains **stateful conversation**
- Displays all intermediate outputs (captions, Wikipedia info, answers, etc.)

---

### Techniques from the Course Applied

- **Multimodal Learning**: Combining text and vision input for richer AI reasoning
- **Transformer Architectures**: BLIP and LLaMA3 are both state-of-the-art transformer-based models
- **Agent-based Reasoning**: The agent acts as an orchestrator using LLM-driven decision-making
- **RAG for Context Injection**: Extending the model’s knowledge using external retrieval (Wikipedia)

## Implementation & Code

This project is implemented as a modular, explainable system. The core logic is written in Python using the following files:

### Code Structure

- **`app.py`** — Streamlit interface to upload images and chat with the AI agent.
- **`agent.py`** — Hosts the agent logic and uses a Groq-hosted LLaMA3 model to decide which tool to invoke.
- **`blip_utils.py`** — Encapsulates vision-language reasoning using BLIP (image captioning, VQA, image-text matching).
- **`rag_utils.py`** — Handles Wikipedia retrieval and open-ended generation via RAG (Retrieval-Augmented Generation).

### Live Demo

You can run the full system locally with:

```bash
streamlit run app.py

### **Tool Selection Prompt**

def decide_tool(user_message):
    prompt = f"""
You are an intelligent tool selector for a visual assistant. Your job is to decide which specialized function (tool) should be used to respond to the user's message, based on the content of their query and the image provided.

You must choose from the following tools:

- caption: Use this when the user wants a simple description or summary of what is visible in the image. Triggers include words like "describe", "what's in this image", or "what do you see".

- vqa: Use this when the user asks a direct question about something specific in the image — such as objects, colors, actions, or quantities. Triggers include "how many", "is there a", "what color", "who is", "where is", "do you see".

- get_info: Use this when the user asks for factual information about something seen in the image (like an object, place, or person) — especially if the assistant should look up external knowledge. Triggers include "tell me about", "what is", "give info on", "Wikipedia".

- compare_caption: Use this to compare the user given caption [this will be a caption for the image] with the one that will be generated by BLIP in our application. Triggers include description or caption of an image, or something like this in the user message "This image is about"

- rag_answer: Use this when the user is asking a deeper or broader question that needs understanding the image **and** do not rely much on the external information, it is usually trash and unrelated. These are usually open-ended questions like "why is this important", "explain this scene", or "what's happening and why" or, Maybe some other questions that do not fall in the above mentioned category.

Rules:
- ALWAYS return exactly one tool name.
- Do NOT explain your reasoning.
- If the input is vague, default to 'caption'.

User message: \"{user_message.strip()}\"
Tool:
"""

The response is parsed to return one of:
- caption
- VQA
- compare_caption
- get_info
- rag_answer





